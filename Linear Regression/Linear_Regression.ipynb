{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPkgc9ZxWc0J23Za7pVXe4U",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tydos/Interpretable-ML-Models/blob/main/Linear%20Regression/Linear_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Regression\n",
        "\n",
        "This notebook implements linear regression using the popular diabetes dataset used for regression"
      ],
      "metadata": {
        "id": "RQdHVcWIkwhs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "qERrDHZSu0ms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = load_diabetes().data\n",
        "y = load_diabetes().target\n",
        "cols = load_diabetes().feature_names\n",
        "\n",
        "print(X[1],y[1],cols)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4yMbvEowbej",
        "outputId": "a78846bd-026e-4bf1-f95f-c9cfe0ca7525"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.00188202 -0.04464164 -0.05147406 -0.02632753 -0.00844872 -0.01916334\n",
            "  0.07441156 -0.03949338 -0.06833155 -0.09220405] 75.0 ['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, ytrain, y_test = train_test_split(X,y,test_size=0.2)\n",
        "print(len(X_train))\n",
        "print(len(X_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEbj_hpywefH",
        "outputId": "99991f66-1a80-4ff6-d3ed-35bb1fcd8387"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "353\n",
            "89\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have to predict y = w*x+b\n",
        "\n",
        "10 features -> 10 weights, 10 biases"
      ],
      "metadata": {
        "id": "RUkgIg6fyBt5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Closed form solution\n",
        "\n",
        "Closed-form solutions are best suited for problems with a small number of features. The dominant computational cost comes from inverting the feature covariance matrix, which has o(n^3) complexity in the number of features and therefore does not scale well to high-dimensional datasets."
      ],
      "metadata": {
        "id": "Z_oJ5Xhdksm5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#closed form solution - this one passes through origin\n",
        "def normal_equation(X, y):\n",
        "    return np.linalg.inv(X.T @ X) @ X.T @ y\n",
        "\n",
        "def normal_bias_equation(X,y):\n",
        "  #add bias using numpy column concat function\n",
        "  len_features = X.shape[0]\n",
        "  bias = np.ones((len_features,1)) #create matrix of size 1*n\n",
        "  X_b = np.c_[bias,X]\n",
        "\n",
        "  theta = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y\n",
        "  return theta\n",
        "\n",
        "def lstsq_normal(X,y):\n",
        "  len_features = X.shape[0]\n",
        "  bias = np.ones((len_features,1)) #create matrix of size 1*n\n",
        "  X_b = np.c_[bias,X]\n",
        "\n",
        "  return np.linalg.lstsq(X_b,y,rcond=None)\n"
      ],
      "metadata": {
        "id": "gsmfo7FWyESb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w = normal_equation(X_train,ytrain)\n",
        "for i, col in enumerate(cols):\n",
        "  print(f\"{col}: {w[i]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNjkVRfb6PP6",
        "outputId": "206eb025-0210-4f67-e99e-af9a406c25fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "age: 49.29676719759565\n",
            "sex: -190.70427659307546\n",
            "bmi: 349.778876623417\n",
            "bp: 500.3243396054219\n",
            "s1: -322.1621020437322\n",
            "s2: 119.1139531044016\n",
            "s3: -61.93438671836239\n",
            "s4: 59.33712029028939\n",
            "s5: 579.1195068364318\n",
            "s6: -41.09220369266277\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w = normal_bias_equation(X_train,ytrain)\n",
        "for i, col in enumerate(cols):\n",
        "  print(f\"{col}: {w[i]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSEnvCxcyaYs",
        "outputId": "1329777a-7e75-45e6-cfdb-9a75abe08b8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "age: 154.77536343077819\n",
            "sex: 6.847805466780149\n",
            "bmi: -227.1055395553644\n",
            "bp: 501.9976219896486\n",
            "s1: 339.1559534072792\n",
            "s2: -733.3300153748287\n",
            "s3: 396.65417189292674\n",
            "s4: 77.49394589362713\n",
            "s5: 197.07479938350198\n",
            "s6: 737.972840441065\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w, residual, rank, s = lstsq_normal(X_train,ytrain)\n",
        "for i, col in enumerate(cols):\n",
        "  print(f\"{col}: {w[i]}\")\n",
        "\n",
        "print(residual)\n",
        "print(rank)\n",
        "print(s)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ErRWN8B40Pyf",
        "outputId": "692505a0-5986-42a0-def8-a04e750d1235"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "age: 154.77536343077747\n",
            "sex: 6.847805466780528\n",
            "bmi: -227.10553955536543\n",
            "bp: 501.9976219896482\n",
            "s1: 339.1559534072782\n",
            "s2: -733.3300153748266\n",
            "s3: 396.6541718929243\n",
            "s4: 77.49394589363055\n",
            "s5: 197.07479938350326\n",
            "s6: 737.9728404410674\n",
            "[961638.07479431]\n",
            "11\n",
            "[18.78846789  1.78902077  1.07394428  0.9725083   0.84022452  0.74022996\n",
            "  0.68110506  0.6432289   0.59536392  0.23543861  0.07739121]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradient descent solution"
      ],
      "metadata": {
        "id": "awsaz4Hjk7ZV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Assumptions of Linear Regression\n",
        "\n",
        "### 1. Linearity (in parameters)\n",
        "**Intuition:**  \n",
        "Each feature contributes additively and proportionally to the prediction.\n",
        "\n",
        "**Violation symptoms:**  \n",
        "- Residuals vs. predicted values show curved patterns  \n",
        "- Systematic under- or over-prediction in certain ranges  \n",
        "\n",
        "**Fixes:**  \n",
        "- Add polynomial or interaction terms  \n",
        "- Transform features (log, square root, etc.)  \n",
        "- Use a non-linear model if necessary  \n",
        "\n",
        "---\n",
        "\n",
        "### 2. Independence of observations\n",
        "**Intuition:**  \n",
        "Each data point should provide new information, not repeat another.\n",
        "\n",
        "**Violation symptoms:**  \n",
        "- Time-series or grouped data (e.g., repeated measurements per user)  \n",
        "- Autocorrelation in residuals  \n",
        "\n",
        "**Fixes:**  \n",
        "- Use time-series models  \n",
        "- Add lag features  \n",
        "- Use clustered/robust standard errors or mixed models  \n",
        "\n",
        "---\n",
        "\n",
        "### 3. Homoscedasticity (constant error variance)\n",
        "**Intuition:**  \n",
        "The model should be equally confident across all prediction ranges.\n",
        "\n",
        "**Violation symptoms:**  \n",
        "- Residual plot shows a fan or cone shape  \n",
        "- Errors increase with the magnitude of predictions  \n",
        "\n",
        "**Fixes:**  \n",
        "- Transform the target variable (log, Box–Cox)  \n",
        "- Use weighted least squares  \n",
        "- Apply heteroscedasticity-robust standard errors  \n",
        "\n",
        "---\n",
        "\n",
        "### 4. Normality of errors (mainly for inference)\n",
        "**Intuition:**  \n",
        "Normally distributed errors allow reliable confidence intervals and hypothesis tests.\n",
        "\n",
        "**Violation symptoms:**  \n",
        "- Skewed or heavy-tailed residual distribution  \n",
        "- Strong deviations from the diagonal in a Q–Q plot  \n",
        "\n",
        "**Fixes:**  \n",
        "- Transform the target variable  \n",
        "- Use bootstrapping methods  \n",
        "- Often ignorable for prediction with large sample sizes  \n",
        "\n",
        "---\n",
        "\n",
        "### 5. No (or low) multicollinearity\n",
        "**Intuition:**  \n",
        "Each feature should explain unique information about the target.\n",
        "\n",
        "**Violation symptoms:**  \n",
        "- Large standard errors for coefficients  \n",
        "- Unstable coefficients or unexpected sign changes  \n",
        "- High Variance Inflation Factor (VIF)  \n",
        "\n",
        "**Fixes:**  \n",
        "- Remove or combine correlated features  \n",
        "- Apply dimensionality reduction (e.g., PCA)  \n",
        "- Use regularization techniques (Ridge, Lasso)  \n",
        "\n"
      ],
      "metadata": {
        "id": "sFcolsKTkao8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Regression model interpretablility"
      ],
      "metadata": {
        "id": "HpVm37Rek-CU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e13A2Not6TIH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}